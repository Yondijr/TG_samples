{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea: Maybe we can skip the untrained layers completely.\n",
    "## Answer: No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, modeling_utils, GPT2Config, modeling_gpt2, GPT2Model, GPT2PreTrainedModel, GPT2Config\n",
    "import pickle\n",
    "from utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(n_layer = 6)\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(\"pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = OrderedDict()\n",
    "chars = [\"6\",\"7\",\"8\",\"9\",\"10\",\"11\"]\n",
    "for c in weights.keys():\n",
    "    var = True\n",
    "    for x in chars:\n",
    "        if x in c:\n",
    "            var = False\n",
    "    if var == True:\n",
    "        out[c] = weights[c]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "lam = open(\"LAMBADA/lambada_control_test_data_plain_text.txt\",\"r+\",encoding=\"utf-8\")\n",
    "lam = lam.read()\n",
    "lam = lam.splitlines()\n",
    "lamT = []\n",
    "for x in lam:\n",
    "    lamT.append(tokenizer.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "GPT_org = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = pickle.load(open(\"LAMBADA/save_preds_fine_1.p\",\"rb\"))\n",
    "step1_corpus = lamT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "out= []\n",
    "for x in range(len(step1)):\n",
    "    final = step1[x][0]\n",
    "    sentence = tokenizer.decode(lamT[x][:-1])\n",
    "    out.append(sentence + final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = split_eos(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = \"The Technology Report empowers or enlightens. ==== The Technology Report empowers or enlightens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "predictions = []\n",
    "for x in eos[:1]: \n",
    "    inp = condition + \" \" + x[-1] + \"====\"\n",
    "    inp = tokenizer.encode(inp) \n",
    "    predictions.append(generation(model,tokenizer,inp))\n",
    "    \n",
    "pickle.dump(predictions,open(\"save_preds_frozen_test.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = pickle.load(open(\"save_preds_frozen_test.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"there weren't many people this high up, which made my search very easy.\",\n",
       " \"they weren't here.\",\n",
       " \"`` let's go up one more level, '' i said, disappointed.\",\n",
       " '`` hurry.',\n",
       " \"'' we tread the rest of the way up the stairs and stood at the edge of the highest level.\",\n",
       " 'this far up, the music was barely a whisper']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"======== \\nâ€œI'm not really looking at any particular particular particular person who would be able accessibly, but rather than give any significant amount of work to the same exacting levels of the same exacting action, which we've been seen in the same exacting stroke. And then we've got a lot of time to go back home to where we've been shownered cleagrificity.======= The same thing happened in order to be a different part of the same thing.<|endoftext|>\"]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
